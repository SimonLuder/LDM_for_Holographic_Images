name: 'ldm_nv_clip'

dataset: # Dataset configuration
  root: "data/"                                                   # root folder of the dataset
  img_channels : 1                                                # grayscale images
  img_size : 200                                                  # original image size
  img_interpolation : 256                                         # size after interpolation or "Null"                   
  labels_train : data/final/poleno/basic_train.csv                # training labels
  labels_val : data/final/poleno/basic_val_20.csv                 # validation labels
  labels_test : data/final/poleno/basic_test.csv                  # test labels
  filenames : rec_path                                            # column name containing filenames
  img_path : img_path                                             # column name containing absolute or relative image path from root
  particle_id : event_id                                          # how to identify the same particle object
  
transforms: # Data transformations
  transform1 : 
    name: "base_transform"
    img_interpolation : 256
  transform2 : 
    name: "clip_image_transform"
    img_interpolation : 224

ddpm: # DDPM architecture
  down_channels : [ 256, 384, 512, 768 ]                          # channels in each downsampling blocks  
  mid_channels : [ 768, 512, 768 ]                                # channels in the middle blocks       
  down_sample : [ True, True, True ]                              # whether to downsample at each block
  attn_down : [True, True, True]                                  # whether to use attention at each block
  time_emb_dim : 512                                              # time embedding dimension
  norm_channels : 32                                              # nr of channels for normalization
  num_heads : 16                                                  # nr of attention heads  
  conv_out_channels : 256                                         # nr of channels after the upsampling blocks
  num_down_layers : 2                                             # nr of layers in each downsampling block
  num_mid_layers : 2                                              # nr of layers in the middle blocks
  num_up_layers : 2                                               # nr of layers in each upsampling block
  cond_emb_dim : 512                                              # conditional embedding dimension
  cond_insert_type : "concatenation"                              # how to combine conditional and time embedding (additive, concatenation or crossattention)

autoencoder: # VQ-VAE
  config_file: checkpoints/vqvae_8_512/slurm_vqvae_config.yaml
  weights: checkpoints/vqvae_8_512/autoencoder/latest.pth

conditioning: # Conditional encoders
  enabled: "clip_image"
  encoders: 

    class: 
      type: categorical
      encoder: Embedding
      use_columns: class_id
      params:
        num_classes: 80
        out_dim: 256

    regionprops: 
      type: numeric_vector
      encoder: MLP
      params:
        in_dim: 15
        hidden_dim: 64
        out_dim: 128
      use_columns:
        - area
        - bbox_area
        - convex_area
        - eccentricity
        - equivalent_diameter
        - feret_diameter_max
        - major_axis_length
        - minor_axis_length
        - max_intensity
        - min_intensity
        - mean_intensity
        - orientation
        - perimeter
        - perimeter_crofton
        - solidity

    clip_image:
      type: image
      encoder: clip_image
      params:
        out_dim: 256
        model_name: "ViT-B/32"
      use_columns: Null
      condition_fn: dual_image_indices

    byol_image:
      type: image
      encoder: byol_image
      params:
        out_dim: 512
        backbone: resnet50
        weights: "checkpoints/to_be_done/last.ckpt"
        emb_layer: "fc"
      use_columns: Null
      condition_fn: dual_image_indices

    rotation:
      type: angle
      encoder: Identity
      params:
        out_dim: 4
      use_columns: Null
      condition_fn: relative_viewpoint_rotation

ldm_train : # LDM training parameters
  seed : 42
  ckpt_folder : 'checkpoints'                                     # everythig saved during the run is here
  num_workers : 4                                                 # nr of workers in the dataloader
  ldm_lr :  0.00005                                               # ldm learning rate
  ldm_disc_lr : 0.00005                                           # discriminator learning rate
  ldm_epochs : 8                                                  # number of epochs to train
  ldm_batch_size : 8                                              # samples per batch
  ldm_steps_per_optimization : 1                                  # optimize every n batches
  ldm_num_timesteps : 1000                                        # use n timesteps to generate the images
  ldm_beta_start : 0.0015                                         # linear noise schedule beta start
  ldm_beta_end : 0.0195                                           # linear noise schedule beta end
  ldm_cfg_discard_prob : 0.1                                      # classifier free guidance discard probability during training
  ldm_cfg_scale : 7.5                                             # classifier free guidance scale during validation 
  ldm_ckpt_steps : 25000                                          # create model checkpoints every n steps
  ldm_val_steps : 25000                                           # validate the model every n steps
  ldm_val_start : 25000                                           # start making checkpoints from this batch onwards
  ldm_perceptual_weight : 0                                       # lpips weight in the loss function
  ldm_discriminator_start_step : -1                               # start the discriminator after n batches (-1 means no discriminator)
  ldm_discriminator_loss : BCEWithLogits                          # discrminator loss criterion
  ldm_discriminator_weight : 0                                    # weight of the discriminator loss
  vqvae_latents_representations : 'vqvae_latents_representations_8_512'
  resume_training: False                                          # if true resume training from a checkpoint                
  resume_optimizer: True                                          # if true resume with optimizer state
  resume_ckpt : checkpoints/ldm_nv_clip/ckpts/latest.ckpt         # if resume_training, use this checkpoint

ddpm_test :
  model_ckpt : 'latest.pth'

ddpm_inference :
  save_images : True
  num_samples : 8
  num_grid_rows : 8
  upscale_latent_dim : True
  conditional : True
  ldm_cfg_scale : 7.5
  ddpm_model_ckpt : 'latest.ckpt'
  
